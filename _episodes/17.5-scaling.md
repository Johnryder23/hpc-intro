---
title: "Scaling"
teaching: 15
exercises: 10
questions:
- "How do we go from running a job on a small number of CPUs to a larger one."
objectives:
- "Understand scaling procedure."
keypoints:
- Start small.
- Test one thing at a time (unit tests).
- Record everything.
---

<!-- TODO: Add scaling example
Currently the rest of this lesson Not ready yet.  Too little info to go on without some sort of easy to grok exercise. -->


The aim of these tests will be to establish how a jobs requirements change with size (CPUs, inputs) and ultimately figure out the best way to run your jobs.

Unfortunately we cannot assume speedup will be linear (e.g. double CPUs won't usually half runtime, doubling the size of your input data won't necessarily double runtime) therefore more testing is required. This is called *scaling testing*.

## Scaling Test

Last time we submitted a job, we did not specify a number of CPUs, and therefore got the default of `2`.

As a reminder, our slurm script `example-job.sl` should currently look like this.

```
{% include example-job.sl.1 %}
```
{: .language-bash}


Using the information we collected from the previous job, we will submit a larger version with our best estimates of required resources.

Now we will add the line `#SBATCH --cpus-per-task 4` to our script.

> ## acctg-freq
>
> By default SLURM records job data every 30 seconds. This means any job running for less than 30 
> seconds will not have it's memory use recorded.
> This recording frequency can be set using `with #SBATCH --acctg-freq 1`, you will not usually need to 
> change this value.
{: .callout}

[//]: # Remove acctg-freq

```
{% include example-job.sl.2 %}
```
{: .language-bash}

And then submit using `sbatch` as we did before.

```
{{ site.remote.prompt }} sbatch example-job.sl
```
{: .language-bash}

{% include {{ site.snippets }}/scheduler/basic-job-script.snip %}

> ## Watch
>
> We can prepend any command with `watch` in order to continuously it. e.g. `watch squeue --me` will 
> give us up to date information on our running jobs.

{: .callout}

> ## OOM Error
>
> Oh no! 
> 
> {% include {{ site.snippets }}/resources/OOM.snip %}
> 
> 1. What went wrong?
> 2. What should be our next steps? 
> > ## Solution
> > The job failed due to an out of "OUT_OF_ME+(MORY)" error. This is because we doubled the number of 
> > CPUs over our previous job, but did not adjust memory.
> > The job running on 2 CPUs used â‰ˆ 200Mb of RAM, extrapolating linearly, we want to give a 4 CPU job 400 Mb + small buffer, say 500Mb to be safe.
> {: .solution}
{: .challenge}

Provided our next job succeeds, we might want to repeat the previous step with more CPUs.

> ## Scaling Exercise
>
> Find your name in the [spreadsheet]({{ site.17_excersice }}) and modify `example-job.sl` to request "x" cpus-per-task.
> Make an estimate memory requirement based on our previous runs. 
> Submit the job with `sbatch example-job.sl`. 
> Watch the job with `squeue --me`.
> On completion of job, use `nn_seff <job-id>` and enter in details in spreadsheet.
> > ## Solution
> > 
> {: .solution}
{: .challenge}

## Scaling Behavior

### Diminishing Returns

Most jobs will look something like this


Under ideal scaling speedup increases 1:1 with number of CPUs. Embarrassingly parallel work will have ideal scaling.

### Amdahl's Law

Most computational tasks will have a certain amount of work that must be computed serially.

[The blue components can be run in parallel, red cannot. Eventually gains will plateau](../fig/AmdahlsLaw.svg)

The fraction of the task that can be run in parallel determines the point of this plateau.
Code that has no serial components is said to be "embarrasingly parallel".

{% include links.md %}
